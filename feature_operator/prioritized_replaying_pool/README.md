# Prioritized Replaying Pool - Number of replaying samples per batch
Section 4.3 of our paper introduces the training process of the Query Potential Estimator (QPE). The following summarizes and expands on this section. We identify three primary causes of estimation bias from uniform sampling: **(1) Neglecting sample importance.** Uniform sampling assigns the same probability to all samples, treating each sample's contribution to model training equally. However, some samples contain crucial information about the model's current errors, while others contribute little to no improvement. **(2) Inefficient use of computational resources.** Uniform sampling distributes computational resources evenly across all samples, causing high-error regions (i.e., where the model performs poorly) to be overlooked, which leads to persistent estimation bias in those regions.  **(3) Ignoring sample occurrence frequency.** The distribution of samples can be highly imbalanced, with certain similar queries appearing frequently within the same workload. Uniform sampling fails to address this imbalance, resulting in the model over-learning certain features.

Therefore, in terms of model training, we are inspired by the prioritized experience mechanism in reinforcement learning [1] and construct a prioritized replaying pool $\mathcal{P}$ to replace the uniform sampling method. We dynamically updates the sampling weight of each sample during training. As training progresses, $\mathcal{P}$ updates the priority of each sample based on the most recent estimation errors, focusing more on samples that are difficult to estimate or have larger errors. This approach enhances the model’s overall performance and accelerates convergence. For specific details regarding the update and replaying step, please refer to the paper. The number of new replay samples $n$ added in each batch serves as a trade-off factor. If $n$ is too large, the model may overfit to high-error samples, resulting in excessively strong corrections and weakening the model's generalization to low-error samples. Conversely, if $n$ is too small, the model may not adequately learn from key high-error samples, slowing the error correction.  Additionally, since the model's performance does not improve much when choosing higher $n$ values, $n = \{b_s}/{20}$ seems to be a reasonable choice.

![image](https://github.com/MaYangrui6/QPE/blob/main/prioritized_replaying_pool/experiment.png)

We conducted an experiment comparing the model's performance under a prioritized replay pool versus uniform sampling, with the results displayed in the figure above. This figure presents the evaluation results for three different workloads. The experiment demonstrates that our proposed training method significantly outperforms uniform sampling across various workloads, particularly in the early stages of training, where the loss decreases rapidly and consistently. This indicates that learning from high-error regions effectively enhances the model's convergence speed. In contrast, the convergence speed of the uniform sampling method is slower and more erratic in the later stages of training, as it distribute low- and high-error samples with equal probability. However, high-error samples typically contribute more to model optimization. Prioritizing these high-error samples can accelerate model convergence and improve generalization performance. Conversely, uniform sampling fails to identify and prioritize these samples, leading to increased fluctuations in the loss curve and instability in model performance due to excessive sampling of easier ones.

[1]: T. Schaul, J. Quan, I. Antonoglou, and D. Silver, “Prioritized experience replay,” arXiv preprint arXiv:1511.05952, 2015.
